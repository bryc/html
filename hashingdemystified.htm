<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Overpass+Mono:wght@300..700&family=Overpass:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
<style>
pre, code {
    font-family: 'Overpass Mono', Consolas, monospace;
    font-weight: 500;
    text-shadow: #fff 0 1px 0;
}
pre {
    color: #14313b;
    background: #eff3f9;
    padding: 12px 20px;
    border-radius: 7px;
}
code {
    color: #004f6a;
    background: #eff3f9;
    font-weight: 600;
    display: inline-block;
    height: 23px;
    line-height: 1.2;
    padding: 0 4px;
    margin: 0 1px;
    outline: 1px solid #eff3f9;
    border-radius: 2px;
}
body {
    color: #222;
    background: #d2e5ff;
    font: 22px 'Overpass', Segoe UI, sans-serif;
    width: 1000px;
    margin: auto;
    margin-bottom: 256px;
}
section {
    background: #fff;
    padding: 5px 28px;
    margin: 54px 0;
    border-radius: 14px;
}
p {
    text-align: justify;
    margin: 12px 0;
    line-height: 1.2;
}
h1, h2, h3 {
    font-weight: 900;
    letter-spacing: -1.4px;
    margin: 12px 0;
}
h1 {
    font-size: 40px;
    border-bottom: 1px solid #e4eaf4;
}
h1 span {
    float:right;
    color: #ccc;
    font-size: 18px;
    font-weight: normal;
    letter-spacing: 1;
}
blockquote {
    color: #777;
    border-left: 7px solid #eee;
    padding-left: 11px;
    margin-left: 18px;
}
a:link    { color:#33f }
a:visited { color:#73f }
a:hover   { filter: brightness(0.5) }

hr{ border: 1px solid #e4eaf4 }

</style>

<section>
<h1>Hashing Demystified<span>github.com/bryc</span></h1>
<!-- With a relatively simple test suite at our disposal -->
<p>In this article we will deconstruct hashing at a conceptual level, exploring theoretical hash functions at various stages of development, identifying what works and what doesn't. The hope is that this may serve as a solid foundation. The code will be in JavaScript, though special attention is made to ensure the syntax is generally C-like.

<h2>But first... What is hashing?</h2>

<p>In the plainest of words, the goal of a <i>hash function</i> is to provide a summary representation of any given input (called a <i>key</i>) as a short, deterministic code—known as the <i>hash</i>. A hash can be compared to that of a fingerprint, but in the form of a <i>number</i> of fixed size. The size of the hash dictates the total amount of possible hashes, and because hashes are finite, it will <i>always</i> be possible for two completely different inputs to produce the same hash. This is mathematically assured by the <a target=_blank href="https://en.wikipedia.org/wiki/Pigeonhole_principle">Pigeonhole principle</a>. And so, ideally, a hash function will attempt to achieve a lower probability of this occurring.

<h2>Why is hashing imporant?</h2>

<p>
Hashing is a key component in many fields. 

<ul>
  <li>It's the backbone of <i>hash tables</i> or <i>hash maps</i>—which are a type of associative array. It allows for looking up a desired value efficiently, rather than having to check every value in the array from top to bottom. You essentially get a direct link to what you're looking for.
  <li>Cryptographic hashes are used for many data security applications, such as password hashing, digital signatures.
  <li>Verifying file integrity.
  <li>File or data identifier.
  <li>As part of other algorithms, such as pseudorandom number generators.
  <li>Converting entropy such as timestamps, into seeds.
</ul>

<h2>Why even read this, is FNV or xxHash bad or something?</h2>

<p>I feel like <i>non-cryptographic</i> hashing is a field where you don't necessarily want to blindly trust claims without testing. It's not like MD5/SHA where a lot of testing has already been done. A lot of code gets spread around on places like StackOverflow; and a lot of it can be bad and still get upvoted. I've seen dreadful random number generators that use <code>Math.PI</code> and <code>Math.sin</code> get praised, and also really poor hash functions lifted from Java and Google Analytics. Just because a hash function works in one case doesn't mean it will work for you; often you end up using it sub-optimally without understanding how or why.

<p>I also see a lot of misinformation spread around, people claiming they know why certain <i>magic numbers</i> were chosen, or why these magic numbers work, but the truth is, they don't.

<p>So I just think being able to test these functions and understand what the operations are doing is important, to actually know what you're doing, so to speak. Even if you don't need to design your own hash, there is value in having a concrete understanding if you are using hash functions in your code.

<p><i>To answer the question: No, FNV isn't bad per se—it's at least better than Java's hashCode, but it's still sub-optimal. xxHash in partcular improves on it in basically every way. But FNV as a quick one-liner is fine in low-stakes situations (or if limited by hardware). You should still take the time to test and compare it to other solutions, though.</i>
</section>

<section>
<h1>Properties of hash functons</h1>

<h2>Uniformity / Collision resistance</h2>

<p>When two different inputs produce the same hash, we call it a <i>collision</i> (bad thing). Under normal circumstances, this cannot be avoided—only mitigated. Each additional bit of data used in the hash calculation will exponentially decrease the probability of collision (good thing). And so the reliability of hashes increases. A 128-bit hash is better than a 64-bit hash is better than a 32-bit hash, and so on.

<p>We can actually model the probabilities of a collision as a solution to the <a target=_blank href="https://en.wikipedia.org/wiki/Birthday_problem">Birthday problem</a>, the name of which comes from the fact that only 23 people need to be in a room for a 50% chance that two people have the same birthday. Which is the same as a collision.

<p>But essentially, if one generates <i>1,000 perfectly-random hashes per second, non-stop</i>, it will take 1 minute for 32-bit hashes to reach a 50% probability of a collision occurring. It will then take 1 month for 64-bit hashes to reach 50%, and 687 million years for a 128-bit hash. The gold standard in cryptography is 256-bit, which would take 13 octillion years to reach a 50% probability.

<p>In other words:
<ul>
  <li>A 50% chance of collision is reached after <b>77,163</b> produced 32-bit hashes.
  <li>That number then becomes <b>5,056,937,541</b> for 64-bit hashes.
  <li>And then <b>21,719,381,355,163,562,492</b> for 128-bit hashes (21.7 quintillion).
</ul>

<p>And so, if you can afford more bits per hash—use them. You probably want to avoid 32-bit hashes if you require more than 1000 unique hashes in a system that cannot handle a single collision. But there are certain scenarios where collision avoidance only matters when comparing a small selection of objects (sometimes just two objects at a time), rather than needing to ensure that millions of objects will have no collision. In which case, 32-bit is fine. In fact, in some cases collisions can be resolved without dire consequences, such as with hash tables that use chaining.

<h2>Checksums and random number generators</h2>

<p>Data corruption has always been a potential problem and is precisely the reason it's always a good idea to keep backups of important data. As such, we have ways of detecting errors in data, such as using checksums. A checksum is a short block of data which is usually stored alongside the data it was calculated from. Upon re-examining the data, the checksum is re-calculated and compared to the stored checksum. If this comparison fails, we know some data corruption has occurred.

<p>Checksums are technically a form of hash with less stringent requirements, due to the fact that it only needs to do a single check on whether the stored hash is equal to the re-calculated hash. Technically in this situation, we actually <i>are</i> looking for a collision; because <i>we must verify that the stored hash is correct</i>, and even a miniscule 8-bit checksum's 1 in 256 chance of a collision, is often reasonable enough assurance that the data is valid. This is because there are 255 hash values that will result in an error, but only one that is a direct match.

<p>Checksums don't need to be algorithmically complex either, as some of the most common errors, such as a flipped bit, can be detected with absolute assurance using a simple sum of each input byte. In fact, hashes which are strongly arithmetic and non-random tend to perform better as checksums, because error-detection rates are more predictable. There are certain specialized checksum algorithms—better described as error-detection algorithms which have even better detection guarantees, such as the famous CRC algorithm. In fact, CRC-32 has often been used as a hash function itself, despite the fact that mathematically speaking, it's an error-detection code. General hash functions typically do not provide any real detection guarantee, and instead rely on probabilities based on the number of bits in the hash result, though in some cases a simple bit flip may be provable to never cause undetectable errors.

<p><b>Pseudo-random number generators</b> also have significant overlap with hash functions. In fact, hash functions and PRNGs can often utilize the same class of <i>mixing functions</i> (aka <i>pseudo-random function</i>) to achieve effective scrambling of the results. For example, MurmurHash3's mixing function has found popular use as a class of PRNG.

<p>Both PRNGs and hashes can exist in varying <i>state sizes</i>, as well as be potentially cryptographically secure (or not). Though there are also considerations that may differ, such as PRNGs needing to maximize its <i>period</i> length; that is, the total amount of subsequent random numbers before the sequence inevitably must repeat. And while collision resistance is important in hash functions, it's not necessarily considered a property of PRNGs.

<p>In any case, the specific details aren't that important, but I think it's valuable to understand that hashing techniques can often translate to other use cases.

<hr>

<p>With all that out of the way, let's begin our hash journey.
</section>

<section>
<h1>A terrible hash function</h1>

<pre>
function Hash(key) {
    let hash = 0;
    for(let i = 0; i < key.length; i++) {
        hash += key[i];
    }
    return hash;
}
</pre>

<p>This is nothing but the sum of all input bytes—known as a <i>checksum</i>. It might do the job of detecting bit errors in a network packet, but has a 99% collision rate as a hash function, which is objectively unusable. Still, it represents a starting point, and even comes with computer trivia:

<p>This exact "algorithm" was listed as a hash function in the first edition of <i>The C Programming Language</i>, by Brian Kernighan and Dennis Ritchie in 1978. To their credit, they did state <i>"This is not the best possible algorithm, but it has
the merit of extreme simplicity".</i>

<h3>Achieving position dependence</h3>

<pre>
function Hash(key) {
    let hash = 0;
    for(let i = 0; i < key.length; i++) {
        hash = hash << 1;
        hash += key[i];
    }
    return hash;
}
</pre>

<p>This is still terrible, but this single change represents a step forward: <i>Position dependence</i>. With the simple summation used previously, <code>[1,2,3]</code> and <code>[3,2,1]</code> would map to the same output (<b>3+2+1</b> is the same as <b>1+2+3</b>), but with an added <i>shift-left</i> operation, the order of the input bytes now change the result. This <i>shift-left</i> operation is equivalent to multiplying by two.

<p>Some historical trivia for you: On microcontrollers and old 8-bit CPUs like the MOS 6502, you can only shift one bit per instruction, so we're holding to that limitation here in tribute. It was the Intel 8086 that introduced variable shift amounts in a single instruction. In the 6502 assembly world, the most efficient checksum would be to instead use a <i>rotate-left</i> instruction with an <i>add-with-carry</i> instruction. Though JS (and almost all languages for that matter) lacks native support for these. 
</section>

<section>
<h1>A bare-minimum hash function</h1>

<pre>
function Hash(key) {
    let hash = 1;
    for(let i = 0; i < key.length; i++) {
        hash += key[i];
        hash += hash << 1;
    }
    return hash;
}
</pre>

<p>Starting with some very subtle changes, we now initialize the hash to <code>1</code> instead of <code>0</code>. This prevents the hash function from doing nothing if the input begins with null bytes. We also swapped the order of operations, and now incorporate the input first. In this case, changing the order won't have a significant effect, but it is generally more effective to incorporate the input <i>as early as possible</i>, so any subsequent mixing operations can maximize their effect. This is also precisely why <a target=_blank href="https://datatracker.ietf.org/doc/html/draft-eastlake-fnv-17.html#section-2">FNV1 was revised to FNV1a</a>; they found that it improved the uniformity of the hash distribution (or avalanche effect). Though we'll get to that later.

<p>The final, most significant change is that we now multiply by 3 instead of 2 (that's what <code>hash += hash << 1</code> is doing; multiplying <code>hash</code> by 2 and then adding to <code>hash</code> is equivalent to <code>hash *= 3</code>). This gives us noticably fewer collisions, but it's still a very poor result.
</section>

<section>
<h1>The over-used, ill-advised hash</h1>

<pre>
function Hash(key) {
    let hash = 0;
    for(let i = 0; i < key.length; i++) {
        hash += hash << 5;
        hash += key[i];
    }
    return hash;
}
</pre>

<p>So, before we move on, this is the perfect time to talk about this popular hash function that still gets spread around. In 1981, James Gosling put this hash function in his version of Emacs (aka Gosling Emacs). In terms of collisions, <i>it's not bad</i>, but still concerningly high when tested against alternatives. What it's doing here is equivalent to multiplying by 33.

<p>More trivia: In 1988 the Second Edition of <i>The C Programming Language</i> was published. That old junk hash function from the previous edition was quietly replaced replaced with this:

<pre>
function Hash(key) {
    let hash = 0;
    for(let i = 0; i < key.length; i++) {
        hash = (hash << 5) - hash;
        hash += key[i];
    }
    return hash;
}
</pre>

<p>It's basically the same as Gosling's hash, but slightly worse. The only difference is that it now multiplies <code>hash</code> by 31 instead of 33 (by doing <code>(hash << 5) - hash</code> instead of <code>(hash << 5) + hash</code>). According to the lore, not even Dennis Richie knew the origin of this particular function when asked.

<p>In 1996, this variant was added to the Java programming language for <code>String.hashCode()</code>. And unfortunately ports of this function have spread like wildfire in modern times due to StackOverflow, despite countless advancements made since then. Apparently the Java developer ran some of his own tests and <a target=_blank href="https://bugs.openjdk.org/browse/JDK-4045622">found 31 superior</a> to 33 in his test, though I am skeptical, as my tests shows it slightly worse or equivalent to 33.

<p>We're still not done though, as yet another famous hash function is nearly identical to these—the djb2 hash from 1990:

<pre>
function Hash(key) {
    let hash = 5381;
    for(let i = 0; i < key.length; i++) {
        hash += hash << 5;
        hash += key[i];
    }
    return hash;
}
</pre>

Aside from the starting value of <code>5381</code>, it's identcal to Gosling's 1981 hash. This was posted on Usenet in 1990, amongst some discussons between Chris Torek & Dan Bernstein. Essentially they were just playing around with the Gosling hash and Bernstein found that starting with <i>5381</i> improved his results. Here's a couple of choice quotes:

<blockquote>Dec 4, 1990 - Bernstein: These days I  start from h = 5381, and set h = h << 5 + h + c mod any power of 2 for each new character c. Apparently Chris Torek prefers a multiplier of 31:  h << 5 - h + c. These are reliable and extremely fast.</blockquote>

<blockquote>June 24, 1991 - Bernstein: Again, practically any good multiplier works. I think you're worrying about the fact that 31c + d doesn't cover any reasonable range of hash values if c and d are between 0 and 255. That's why, when I discovered the 33 hash function and started using it in my compressors, I started with a hash value of 5381. I think you'll find that this does just as well as a 261 multiplier.</blockquote>

<blockquote>June 26, 1991 - Bernstein: The profiling statistics I've saved from various compressors form a much more realistic sample. They show 33 as slightly better than random hashing on typical files, 37 as slightly worse. I've never tried 261 or 31, but I'll bet 261 does worse than 33 on text files.</blockquote>

<p>In any case, I've yet to find a measurable result where the starting value of <code>5381</code> has any meaningful improvement over say, <code>9</code>. There is one last variant of note before we move on.

<pre>
function Hash(key) {
    let hash = 5381;
    for(let i = 0; i < key.length; i++) {
        hash += hash << 5;
        hash ^= key[i];
    }
    return hash;
}
</pre>

<p>This is Bernstein's "cdb" hash from 1996, also known as djb2a. Its singular difference is that it uses XOR instead of ADD for the input. This change does seem to result in measurably fewer collisions, though not enough to make it a "good" hash function. This benefit seems to be directly tied to the usage of the 33 multiplier.
</section>

<section>
<h1>The best single-shift hash</h1>

<pre>
function Hash(key) {
    let hash = 1;
    for(let i = 0; i < key.length; i++) {
        hash += key[i];
        hash += hash << 7;
    }
    return hash;
}
</pre>

<p>And so, combining everything we've learned so far, gets us here. We're now multiplying by 129 instead of 33. <code>7</code> seems to be the optimal shift value that results in the lowest collision rate so far. However it's still not competitive.

<p>To be continued.
</section>




<style>
pre b {color: #7B3C00} 
pre x {color: #468100} 
pre z {color: #1880F7}
</style>
<script>
document.querySelectorAll('pre').forEach(function(el) {
    // ghetto syntax highlighting
    el.innerHTML = el.innerHTML.replaceAll(/(function|for|let|return)/g, "<b>$1</b>");
    el.innerHTML = el.innerHTML.replaceAll(/(hash|key)/g, "<x>$1</x>");
    el.innerHTML = el.innerHTML.replaceAll(/(\d+)/g, "<z>$1</z>");
});
</script>
